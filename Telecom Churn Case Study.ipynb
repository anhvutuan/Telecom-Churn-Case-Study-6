{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom Churn Case Study Kaggle Competition\n",
    "## Problem Statement\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    "For many incumbent operators, retaining high profitable customers is the number one business\n",
    "goal. To reduce customer churn, telecom companies need to predict which customers are at high risk of churn. In this project, you will analyze customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn.\n",
    "\n",
    "In this competition, your goal is to build a machine learning model that is able to predict churning customers based on the features provided for their usage.\n",
    "## Objectives\n",
    "Objectives\n",
    "The main goal of the case study is to build ML models to predict churn. The predictive model that youâ€™re going to build will the following purposes:\n",
    "\n",
    "- It will be used to predict whether a high-value customer will churn or not, in near future (i.e. churn phase). By knowing this, the company can take action steps such as providing special plans, discounts on recharge etc.\n",
    "\n",
    "- It will be used to identify important variables that are strong predictors of churn. These variables may also indicate why customers choose to switch to other networks.\n",
    "\n",
    "- Even though overall accuracy will be your primary evaluation metric, you should also mention other metrics like precision, recall, etc. for the different models that can be used for evaluation purposes based on different business objectives. For example, in this problem statement, one business goal can be to build an ML model that identifies customers who'll definitely churn with more accuracy as compared to the ones who'll not churn. Make sure you mention which metric can be used in such scenarios.\n",
    "\n",
    "- Recommend strategies to manage customer churn based on your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.express as px \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from xgboost import XGBClassifier,XGBRFClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,precision_recall_curve,PrecisionRecallDisplay,roc_auc_score,roc_curve\n",
    "import optuna\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing display limit of dataframe (optional cell to run)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Setting style for seaonrn\n",
    "sns.color_palette(\"seismic\", 50)\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing train dataset and displaying first 5 rows\n",
    "tel_churn=pd.read_csv('train.csv')\n",
    "tel_churn.head()\n",
    "\n",
    "# Importing train dataset and displaying first 5 rows\n",
    "tel_churn_test=pd.read_csv('test.csv')\n",
    "tel_churn_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of dataset\n",
    "tel_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about dataset\n",
    "tel_churn.info(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic information about the data\n",
    "## Number of rows and columns\n",
    "print('Number of Columns:',tel_churn.shape[1])\n",
    "print('Number of Rows:',tel_churn.shape[0])\n",
    "## Number of missing values\n",
    "print('Number of missing values:',tel_churn.isnull().sum().sum())\n",
    "## Number of unique values\n",
    "print('Number of unique values:',tel_churn.nunique().sum())\n",
    "## Number of duplicates\n",
    "print('Number of duplicates:',tel_churn.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Distribution\n",
    "tel_churn.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Classes Distribution\n",
    "tel_churn['churn_probability'].value_counts()/tel_churn.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From describe() and info() we can see that data has large number of null values as well as outliers which needs to be handled. We will handle these issues in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing value percentage\n",
    "pd.DataFrame((tel_churn.isnull().sum()/len(tel_churn)*100).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing column with 30% or more null values as it will reduce the impact on analysis\n",
    "tel_churn = tel_churn.loc[:,tel_churn.isnull().sum()/tel_churn.shape[0]*100<30]\n",
    "# Shape of the dataframe after removing columns\n",
    "tel_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing value percentage\n",
    "pd.DataFrame((tel_churn.isnull().sum()/len(tel_churn)*100).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with missing values with more than 10 missing values\n",
    "tel_churn.dropna(axis=0,inplace=True,thresh=tel_churn.shape[1]-10)\n",
    "# Shape of the dataframe after removing rows\n",
    "print(tel_churn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns with Date datatype\n",
    "date_cols = [k for k in tel_churn.columns.to_list() if 'date' in k]\n",
    "date_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Date columns to datetime datatype and extracting the days before last day\n",
    "for i in date_cols:\n",
    "    tel_churn[i] = pd.to_datetime(tel_churn[i])\n",
    "    tel_churn[i] = tel_churn[i].dt.date\n",
    "    tel_churn[i] = pd.to_datetime(tel_churn[i])\n",
    "    tel_churn[i] = tel_churn[i].dt.daysinmonth - tel_churn[i].dt.day\n",
    "\n",
    "# for test data set\n",
    "for i in date_cols:\n",
    "    tel_churn_test[i] = pd.to_datetime(tel_churn_test[i])\n",
    "    tel_churn_test[i] = tel_churn_test[i].dt.date\n",
    "    tel_churn_test[i] = pd.to_datetime(tel_churn_test[i])\n",
    "    tel_churn_test[i] = tel_churn_test[i].dt.daysinmonth - tel_churn_test[i].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the date columns\n",
    "tel_churn[date_cols].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the columns with only one unique value among date columns\n",
    "tel_churn.drop(columns=['last_date_of_month_6','last_date_of_month_7','last_date_of_month_8'],inplace=True)\n",
    "date_cols.remove('last_date_of_month_6')\n",
    "date_cols.remove('last_date_of_month_7')\n",
    "date_cols.remove('last_date_of_month_8')\n",
    "\n",
    "# for test data set\n",
    "tel_churn_test.drop(columns=['last_date_of_month_6','last_date_of_month_7','last_date_of_month_8'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate ID columns\n",
    "tel_churn.drop(['circle_id'],axis=1,inplace=True)\n",
    "# for test data set\n",
    "tel_churn_test.drop(['circle_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(tel_churn,figsize=(20,10),fontsize=12,color=(0.42, 0.1, 0.05),sparkline=True,labels=True,label_rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is evident from the chart there is some missing values in the data. We will try to find the pattern and fill the missing values accordingly. We are using MICE technique to fill the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MICE to impute missing values\n",
    "imp = IterativeImputer(estimator=BayesianRidge(),max_iter=10, random_state=0)\n",
    "# Fitting the imputer for each index in date columns\n",
    "for i in date_cols:\n",
    "    tel_churn[i] = imp.fit_transform(tel_churn[i].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the missing values are filled and thus can proceed with the next step of data analysis. As all columns are numeric we can proceed with scatter charts to find the correlation between the columns. One last step to remove unwanted columns from the data which have only one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Columns with only one unique value\n",
    "tel_churn = tel_churn.loc[:,tel_churn.nunique()!=1]\n",
    "tel_churn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average recharge amount for June and July\n",
    "tel_churn['avg_rech_amt_6_7']=((tel_churn['total_rech_amt_6']+tel_churn['total_rech_amt_7'])/2)\n",
    "# for test data set\n",
    "tel_churn_test['avg_rech_amt_6_7']=((tel_churn_test['total_rech_amt_6']+tel_churn_test['total_rech_amt_7'])/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days user with company\n",
    "tel_churn['days_stayed'] = tel_churn['date_of_last_rech_8'] - tel_churn['date_of_last_rech_6']\n",
    "# for test data set\n",
    "tel_churn_test['days_stayed'] = tel_churn_test['date_of_last_rech_8'] - tel_churn_test['date_of_last_rech_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average 3g usage for June and July\n",
    "tel_churn['avg_3g_6_7']=((tel_churn['vol_3g_mb_6']+tel_churn['vol_3g_mb_7'])/2)\n",
    "# for test data set\n",
    "tel_churn_test['avg_3g_6_7']=((tel_churn_test['vol_3g_mb_6']+tel_churn_test['vol_3g_mb_7'])/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average 2g usage for June and July\n",
    "tel_churn['avg_2g_6_7']=((tel_churn['vol_2g_mb_6']+tel_churn['vol_2g_mb_7'])/2)\n",
    "# for test data set\n",
    "tel_churn_test['avg_2g_6_7']=((tel_churn_test['vol_2g_mb_6']+tel_churn_test['vol_2g_mb_7'])/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avergae of 6th and 7th month total usage\n",
    "tel_churn['avg_total_6_7']=((tel_churn['total_og_mou_6']+tel_churn['total_og_mou_7'])/2)\n",
    "# for test data set\n",
    "tel_churn_test['avg_total_6_7']=((tel_churn_test['total_og_mou_6']+tel_churn_test['total_og_mou_7'])/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg. mou at action phase\n",
    "# We are taking average because there are two months(7 and 8) in action phase\n",
    "tel_churn['avg_mou_action'] = (tel_churn['total_og_mou_7'] + tel_churn['total_og_mou_8'] + tel_churn['total_ic_mou_7'] + tel_churn['total_ic_mou_8'])/2\n",
    "# for test data set\n",
    "tel_churn_test['avg_mou_action'] = (tel_churn_test['total_og_mou_7'] + tel_churn_test['total_og_mou_8'] + tel_churn_test['total_ic_mou_7'] + tel_churn_test['total_ic_mou_8'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARUP in action phase\n",
    "tel_churn['avg_arpu_action'] = (tel_churn['arpu_7'] + tel_churn['arpu_8'])/2\n",
    "# Difference of good and action phase ARPU\n",
    "tel_churn['diff_arpu'] = tel_churn['avg_arpu_action'] - tel_churn['arpu_6']\n",
    "# Checking whether the arpu has decreased on the action month\n",
    "tel_churn['decrease_arpu_action'] = np.where(tel_churn['diff_arpu'] < 0, 1, 0)\n",
    "\n",
    "# ARUP in action phase\n",
    "tel_churn_test['avg_arpu_action'] = (tel_churn_test['arpu_7'] + tel_churn_test['arpu_8'])/2\n",
    "# Difference of good and action phase ARPU\n",
    "tel_churn_test['diff_arpu'] = tel_churn_test['avg_arpu_action'] - tel_churn_test['arpu_6']\n",
    "# Checking whether the arpu has decreased on the action month\n",
    "tel_churn_test['decrease_arpu_action'] = np.where(tel_churn_test['diff_arpu'] < 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering High-Value Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the customers based on average recharge amount\n",
    "perc_6_7=tel_churn['avg_rech_amt_6_7'].quantile(0.70)\n",
    "tel_churn=tel_churn[tel_churn['avg_rech_amt_6_7']>=perc_6_7]\n",
    "tel_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column list with id and target variable\n",
    "col_list_bar = tel_churn.columns.to_list()\n",
    "col_list_bar.remove('churn_probability')\n",
    "col_list_bar.remove('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outlier using z-score method \n",
    "z = np.abs(stats.zscore(tel_churn[col_list_bar]))\n",
    "tel_churn = tel_churn[(z < 5).all(axis=1)]\n",
    "tel_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for outliers using boxplot in a 34 by 4 grid\n",
    "fig, ax = plt.subplots(34, 4, figsize=(20, 60))\n",
    "for variable, subplot in zip(col_list_bar, ax.flatten()):\n",
    "    tel_churn.boxplot(column=variable, ax=subplot, vert=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the outluers are present in large number and IQR method will result in loss of data we used z score method. As we are using a model to predict the churn we can use a model which is robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating copy of dataframe for Predictive model\n",
    "tel_churn_pred = tel_churn.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uni-variate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for distribution using distplot in a 34 by 4 grid\n",
    "fig, ax = plt.subplots(34, 4, figsize=(20, 100))\n",
    "for variable, subplot in zip(col_list_bar, ax.flatten()):\n",
    "    sns.distplot(tel_churn[variable], ax=subplot,kde=True, bins=50, hist_kws={'alpha': 1})\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-variate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis of churn probability with other variables\n",
    "fig, ax = plt.subplots(34, 4, figsize=(20, 100))\n",
    "for variable, subplot in zip(col_list_bar, ax.flatten()):\n",
    "    sns.boxplot(x='churn_probability', y=variable, data=tel_churn, ax=subplot)\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap with target variable\n",
    "plt.figure(figsize=(20,30))\n",
    "sns.heatmap(tel_churn_pred.corr()[['churn_probability']].sort_values(by='churn_probability',ascending=False),cmap='coolwarm',annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using a model to predict the churn we can use a model which is robust to multi-collinearity and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building for Predicting Churn OBJECTIVE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X = tel_churn_pred.drop(['id','churn_probability'],axis=1)\n",
    "y = tel_churn_pred['churn_probability']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=71, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Combined sampling to handle imbalanced dataset\n",
    "smt = SMOTETomek(random_state=71, sampling_strategy=0.25, n_jobs=-1)\n",
    "X_train, y_train = smt.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization method\n",
    "scaler = MinMaxScaler()\n",
    "X_train[col_list_bar] = scaler.fit_transform(X_train[col_list_bar])\n",
    "X_test[col_list_bar] = scaler.transform(X_test[col_list_bar])\n",
    "tel_churn_test[col_list_bar] = scaler.transform(tel_churn_test[col_list_bar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 9),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
    "        'subsample': trial.suggest_loguniform('subsample', 0.01, 1.0),\n",
    "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'use_label_encoder':[ False,True],\n",
    "    }\n",
    "\n",
    "    # Fit the model\n",
    "    optuna_model = XGBRFClassifier(**params, random_state=71,tree_method='gpu_hist', gpu_id=0)\n",
    "    optuna_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = optuna_model.predict(X_test)\n",
    "\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of finished trials: {len(study.trials)}')\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f'  Value: {trial.value}')\n",
    "print('  Params: ')\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = trial.params\n",
    "model = XGBRFClassifier(**params, random_state=71)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,y_pred_train))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_train).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "sensitivity = tp / (tp+fn)\n",
    "print('Specificity:',specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output for Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the predicted the class for test data\n",
    "output=pd.DataFrame({\"id\":tel_churn_test.id,\"churn_probability\":model.predict(tel_churn_test[X_train.columns])})\n",
    "output.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking submission accuracy\n",
    "solution=pd.read_csv(\"solution.csv\")\n",
    "solution.head()\n",
    "accuracy_score(solution.churn_probability,output.churn_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building for feature importance OBJECTIVE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making copy for feature importance\n",
    "tel_churn_features = tel_churn.copy()\n",
    "X_feature=tel_churn_features[col_list_bar]\n",
    "y_feature=tel_churn_features['churn_probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Combined sampling to handle imbalanced dataset\n",
    "smt = SMOTETomek(random_state=40, sampling_strategy=0.2, n_jobs=-1)\n",
    "X_feature, y_feature = smt.fit_resample(X_feature, y_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization method\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_feature[col_list_bar] = scaler.fit_transform(X_feature[col_list_bar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature importance\n",
    "model = XGBClassifier(random_state=80)\n",
    "model.fit(X_feature, y_feature)\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "feature_imp=pd.DataFrame({\"feature\":X_feature.columns,\"importance\":importance/np.sum(importance)*100})\n",
    "feature_imp.sort_values(by='importance',ascending=False,inplace=True)\n",
    "feature_imp=feature_imp[feature_imp.importance>0]\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features above 30% quantile for feature importance\n",
    "feature_imp_50=feature_imp[feature_imp.importance>feature_imp.importance.quantile(0.3)]\n",
    "feature_imp_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting feature importance\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(x=feature_imp_50.feature,y=feature_imp_50.importance)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most important 20 features\n",
    "feature_imp[feature_imp.importance>feature_imp.importance.quantile(0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting top 20 features for churn and no churn using distplot\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in range(1,21):\n",
    "    plt.subplot(5,4,i)\n",
    "    sns.distplot(tel_churn[tel_churn['churn_probability']==1][feature_imp[feature_imp.importance>feature_imp.importance.quantile(0.8)].feature.values[i-1]],label='Churn')\n",
    "    sns.distplot(tel_churn[tel_churn['churn_probability']==0][feature_imp[feature_imp.importance>feature_imp.importance.quantile(0.8)].feature.values[i-1]],label='No Churn')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recomendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The compÐ°ny should focus on users with lower roÐ°ming outgoing cÐ°lls in August by providing them with better plÐ°ns.\n",
    "- SimilÐ°rly, the compÐ°ny should focus on users with lower roÐ°ming incoming cÐ°lls in August by providing them with better plÐ°ns.\n",
    "- TÐ°rget the customers, whose minutes of usÐ°ge of the incoming locÐ°l cÐ°lls Ð°nd outgoing ISD cÐ°lls Ð°re less in the Ð°ction phÐ°se (mostly in the month of August).\n",
    "- TÐ°rget the customers, whose outgoing others chÐ°rge in July Ð°nd incoming others in August Ð°re less.\n",
    "- Also, the customers hÐ°ving vÐ°lue-bÐ°sed cost in the Ð°ction phÐ°se increÐ°sed Ð°re more likely to churn thÐ°n the other customers. Hence, these customers mÐ°y be Ð° good tÐ°rget to provide offers.\n",
    "- Customers, whose monthly 3G rechÐ°rge in August is more, Ð°re likely to be churned.\n",
    "- Customers hÐ°ving to decreÐ°se STD incoming minutes of usÐ°ge for operÐ°tors T to fixed lines of T for the month of August Ð°re more likely to churn.\n",
    "- Customers decreÐ°sing monthly 2g usÐ°ge for August Ð°re most probÐ°ble to churn.\n",
    "- Customers hÐ°ving decreÐ°sing incoming minutes of usÐ°ge for operÐ°tors T to fixed lines of T for August Ð°re more likely to churn.\n",
    "- roÐ°m_og_mou_8 vÐ°riÐ°bles hÐ°ve positive coefficients. ThÐ°t meÐ°ns thÐ°t customers, whose roÐ°ming outgoing minutes of usÐ°ge Ð°re increÐ°sing Ð°re more likely to churn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
